{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Iceberg Hands-on Lab: Cafe Chain Data Management\n",
    "\n",
    "Software Engineering, Shenkar - BDE Course L6\n",
    "\n",
    "In this lab, you'll work with Apache Iceberg to manage data for GlobalCafe, a multinational coffee chain. You'll learn how to:\n",
    "- Set up an Iceberg environment\n",
    "- Create and manage tables\n",
    "- Handle schema evolution\n",
    "- Perform time travel queries\n",
    "- Implement data maintenance tasks\n",
    "\n",
    "## Prerequisites\n",
    "- Docker and Docker Compose installed\n",
    "- Basic understanding of SQL and PySpark\n",
    "- Git installed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Environment Setup\n",
    "\n",
    "First, let's set up our environment. Run these commands in your terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ngit clone https://github.com/your-repo/cafe-iceberg-lab\\ncd cafe-iceberg-lab\\ndocker-compose up -d\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run these commands in your terminal, not in this notebook\n",
    "'''\n",
    "git clone https://github.com/your-repo/cafe-iceberg-lab\n",
    "cd cafe-iceberg-lab\n",
    "docker-compose up -d\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's initialize our PySpark session with Iceberg support:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      3\u001b[0m spark \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39mbuilder \\\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCafeIcebergLab\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.jars.packages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morg.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;241m.\u001b[39mconfig(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.catalog.local.warehouse\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/warehouse/cafe\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;241m.\u001b[39mgetOrCreate()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CafeIcebergLab\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.4_2.12:1.4.2\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.iceberg.spark.SparkSessionCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"/warehouse/cafe\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Creating Tables\n",
    "\n",
    "Let's create tables for our cafe chain data model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create database\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCREATE DATABASE IF NOT EXISTS cafe_ops\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUSE cafe_ops\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Create sales transactions table\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# Create database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS cafe_ops\")\n",
    "spark.sql(\"USE cafe_ops\")\n",
    "\n",
    "# Create sales transactions table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sales_transactions (\n",
    "    transaction_id BIGINT,\n",
    "    store_id BIGINT,\n",
    "    cashier_id BIGINT,\n",
    "    transaction_timestamp TIMESTAMP,\n",
    "    total_amount DECIMAL(10,2),\n",
    "    payment_method STRING,\n",
    "    items ARRAY<STRUCT<\n",
    "        item_id: BIGINT,\n",
    "        quantity: INT,\n",
    "        unit_price: DECIMAL(10,2)\n",
    "    >>\n",
    ") USING iceberg\n",
    "PARTITIONED BY (days(transaction_timestamp))\n",
    "\"\"\")\n",
    "\n",
    "# Create store locations table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS store_locations (\n",
    "    store_id BIGINT,\n",
    "    store_name STRING,\n",
    "    city STRING,\n",
    "    country STRING,\n",
    "    opening_date DATE,\n",
    "    store_type STRING,\n",
    "    square_footage INT\n",
    ") USING iceberg\n",
    "\"\"\")\n",
    "\n",
    "# Create menu items table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS menu_items (\n",
    "    item_id BIGINT,\n",
    "    item_name STRING,\n",
    "    category STRING,\n",
    "    price DECIMAL(10,2),\n",
    "    is_seasonal BOOLEAN,\n",
    "    available_from DATE,\n",
    "    available_until DATE,\n",
    "    calories INT\n",
    ") USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Insert Sample Data\n",
    "\n",
    "Insert the following sample data into your tables. Use both SQL and DataFrame APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+--------+-------+------------+----------+--------------+\n",
      "|store_id|        store_name|    city|country|opening_date|store_type|square_footage|\n",
      "+--------+------------------+--------+-------+------------+----------+--------------+\n",
      "|       1|    Downtown Plaza|New York|    USA|  2020-01-15|  flagship|          2500|\n",
      "|       2|Airport Terminal 3|  London|     UK|  2021-03-20|     kiosk|           800|\n",
      "|       3|     Shopping Mall|   Tokyo|  Japan|  2019-11-30|  standard|          1500|\n",
      "+--------+------------------+--------+-------+------------+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "# Define the schema explicitly\n",
    "schema = StructType([\n",
    "    StructField(\"store_id\", LongType(), False),\n",
    "    StructField(\"store_name\", StringType(), False),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"country\", StringType(), False),\n",
    "    StructField(\"opening_date\", DateType(), False),\n",
    "    StructField(\"store_type\", StringType(), False),\n",
    "    StructField(\"square_footage\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "# Convert string dates to Python date objects\n",
    "stores_data = [\n",
    "    (1, \"Downtown Plaza\", \"New York\", \"USA\", date(2020, 1, 15), \"flagship\", 2500),\n",
    "    (2, \"Airport Terminal 3\", \"London\", \"UK\", date(2021, 3, 20), \"kiosk\", 800),\n",
    "    (3, \"Shopping Mall\", \"Tokyo\", \"Japan\", date(2019, 11, 30), \"standard\", 1500)\n",
    "]\n",
    "\n",
    "# Create DataFrame with schema\n",
    "stores_df = spark.createDataFrame(stores_data, schema=schema)\n",
    "\n",
    "# Now append to the table\n",
    "stores_df.writeTo(\"cafe_ops.store_locations\").append()\n",
    "stores_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore MinIO\n",
    "\n",
    "http://localhost:9001/browser/warehouse/cafe_ops/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 1.1\n",
    "# Insert menu items using SQL\n",
    "# Hint: Use spark.sql() with INSERT INTO statement\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Schema Evolution\n",
    "\n",
    "Add nutritional information to the menu items table and update existing records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 2.1\n",
    "# Add new columns for nutritional info\n",
    "# Update existing records with the new information\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Time Travel Queries\n",
    "\n",
    "Perform time travel queries to explore different versions of your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 3.1\n",
    "# 1. Query the current state of menu_items\n",
    "# 2. Make some changes to the data\n",
    "# 3. Query a previous version using timestamp\n",
    "# 4. Query using snapshot ID\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Data Maintenance\n",
    "\n",
    "Implement data maintenance tasks including snapshot expiration and file compaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercise 4.1\n",
    "# 1. Expire old snapshots\n",
    "# 2. Rewrite data files (compaction)\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Available tables:\")\n",
    "spark.sql(\"SHOW TABLES FROM cafe_ops\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus Exercise: Data Quality and Analytics\n",
    "\n",
    "Implement data quality checks and write analytical queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bonus 1\n",
    "# Implement data quality checks:\n",
    "# 1. Check for negative prices\n",
    "# 2. Check for future dates\n",
    "# 3. Validate store_ids\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Bonus 2\n",
    "# Write analytics queries:\n",
    "# 1. Top selling items by revenue\n",
    "# 2. Sales patterns by store type\n",
    "# 3. Seasonal item performance\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission Questions\n",
    "\n",
    "Please answer the following questions:\n",
    "\n",
    "1. How does Iceberg handle schema evolution differently from traditional databases?\n",
    "2. What are the benefits of time travel in this cafe chain context?\n",
    "3. How would you implement a data retention policy using Iceberg features?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
